# 100 Days of Machine Learning

Welcome to my 100 Days of Machine Learning journey! This README will serve as a daily log and resource guide as I progress through various topics and projects in machine learning, with a focus on using PyTorch. Today is **Day 40**

---

## Day 40:

---

## Day 39: Finalized with intermediate docker concepts

---

## Day 38: Deep Dive into Docker

---

## Day 37: Docker and Kubernetes

---

## Day 36: Continuation with ResNet

```bash
(lmri) leo@mcbills:~/Desktop/100_days/100_days-ML/models$ python3 resnet.py 
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 112, 112]           9,472
       BatchNorm2d-2         [-1, 64, 112, 112]             128
              ReLU-3         [-1, 64, 112, 112]               0
         MaxPool2d-4           [-1, 64, 56, 56]               0
            Conv2d-5           [-1, 64, 56, 56]          36,928
       BatchNorm2d-6           [-1, 64, 56, 56]             128
              ReLU-7           [-1, 64, 56, 56]               0
            Conv2d-8           [-1, 64, 56, 56]          36,928
       BatchNorm2d-9           [-1, 64, 56, 56]             128
             ReLU-10           [-1, 64, 56, 56]               0
    ResidualBlock-11           [-1, 64, 56, 56]               0
           Conv2d-12           [-1, 64, 56, 56]          36,928
      BatchNorm2d-13           [-1, 64, 56, 56]             128
             ReLU-14           [-1, 64, 56, 56]               0
           Conv2d-15           [-1, 64, 56, 56]          36,928
      BatchNorm2d-16           [-1, 64, 56, 56]             128
             ReLU-17           [-1, 64, 56, 56]               0
    ResidualBlock-18           [-1, 64, 56, 56]               0
           Conv2d-19          [-1, 128, 28, 28]          73,856
      BatchNorm2d-20          [-1, 128, 28, 28]             256
             ReLU-21          [-1, 128, 28, 28]               0
           Conv2d-22          [-1, 128, 28, 28]         147,584
      BatchNorm2d-23          [-1, 128, 28, 28]             256
           Conv2d-24          [-1, 128, 28, 28]           8,320
      BatchNorm2d-25          [-1, 128, 28, 28]             256
             ReLU-26          [-1, 128, 28, 28]               0
    ResidualBlock-27          [-1, 128, 28, 28]               0
           Conv2d-28          [-1, 128, 28, 28]         147,584
      BatchNorm2d-29          [-1, 128, 28, 28]             256
             ReLU-30          [-1, 128, 28, 28]               0
           Conv2d-31          [-1, 128, 28, 28]         147,584
      BatchNorm2d-32          [-1, 128, 28, 28]             256
             ReLU-33          [-1, 128, 28, 28]               0
    ResidualBlock-34          [-1, 128, 28, 28]               0
           Conv2d-35          [-1, 256, 14, 14]         295,168
      BatchNorm2d-36          [-1, 256, 14, 14]             512
             ReLU-37          [-1, 256, 14, 14]               0
           Conv2d-38          [-1, 256, 14, 14]         590,080
      BatchNorm2d-39          [-1, 256, 14, 14]             512
           Conv2d-40          [-1, 256, 14, 14]          33,024
      BatchNorm2d-41          [-1, 256, 14, 14]             512
             ReLU-42          [-1, 256, 14, 14]               0
    ResidualBlock-43          [-1, 256, 14, 14]               0
           Conv2d-44          [-1, 256, 14, 14]         590,080
      BatchNorm2d-45          [-1, 256, 14, 14]             512
             ReLU-46          [-1, 256, 14, 14]               0
           Conv2d-47          [-1, 256, 14, 14]         590,080
      BatchNorm2d-48          [-1, 256, 14, 14]             512
             ReLU-49          [-1, 256, 14, 14]               0
    ResidualBlock-50          [-1, 256, 14, 14]               0
           Conv2d-51            [-1, 512, 7, 7]       1,180,160
      BatchNorm2d-52            [-1, 512, 7, 7]           1,024
             ReLU-53            [-1, 512, 7, 7]               0
           Conv2d-54            [-1, 512, 7, 7]       2,359,808
      BatchNorm2d-55            [-1, 512, 7, 7]           1,024
           Conv2d-56            [-1, 512, 7, 7]         131,584
      BatchNorm2d-57            [-1, 512, 7, 7]           1,024
             ReLU-58            [-1, 512, 7, 7]               0
    ResidualBlock-59            [-1, 512, 7, 7]               0
           Conv2d-60            [-1, 512, 7, 7]       2,359,808
      BatchNorm2d-61            [-1, 512, 7, 7]           1,024
             ReLU-62            [-1, 512, 7, 7]               0
           Conv2d-63            [-1, 512, 7, 7]       2,359,808
      BatchNorm2d-64            [-1, 512, 7, 7]           1,024
             ReLU-65            [-1, 512, 7, 7]               0
    ResidualBlock-66            [-1, 512, 7, 7]               0
AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0
           Linear-68                   [-1, 10]           5,130
================================================================
Total params: 11,186,442
Trainable params: 11,186,442
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 62.79
Params size (MB): 42.67
Estimated Total Size (MB): 106.03
----------------------------------------------------------------
```
---

## Day 35: A deep dive into the ResNet architecture

---

## Day 34:

---

## Day 33:

---

## Day32: Write a script for custom datasets for an image enhancing project

---

## Day 31: Practice with organizing own datasets

---

## Day 30:

---

## Day 29:

---

## Day 28:
---

## Day 27: Logging and progress bars with python

---

## Day 26: Storage Classes and Namespaces
This chapter begins by describing storage classes for objects and functions. The storage class is responsible for defining those parts of a program where an object or function can be used. Namespaces can be used to avoid conflicts when naming global identifiers.

---

## Day 25: Macros in c++

## Converting Arithmetic Types
This chapter introduces implicit type conversions, which are performed in c++ whenever different arithmetic types occur in expressions.
Additionally, an operator for explicit type conversion is introduced.

## The standard Class string
This chapter introduces the standard class string, which is used to represent strings. Besides defining strings we will also look at various methods of string manipulation. These include inserting and erasing, searching and replacing, comparing and concatenating strings.

## Functions
This chapter describes how to write functions of your own. Besides the basic rules, the following topics are discussed:
- passing arguments
- definition of iniline functions
- overloading functions and default arguments
- the principle of recursion

---

## Day 24: Deep dive into control statements


---

## Day 23: Operators for Fundamental Types
Today, I shall look at operators needed for calculations and selections are introduced. Overloading and other operators, such as those needed for bit manipulations, are gonna be tackled later.

## Control Flow
In this chapter, I was introduced to the statements needed to control the flow of a program. These are;
* loops with while, do-while, and for
* selections with if-else, switch, and the conditional operator
* jumps with goto, continue, and break

---

## Day 22: Functions and classes
I looked at functions and classes in c++ basically but did not look at user defined classes

---

## Day 21: Introduction to C++ programming

### How to run c++ in my terminal
1. Write the program and save it as an `.cpp` program
2. Compile the program using the g++ compiler by;
```bash
g++ -o execfile program.cpp
```
3. Run the executable file
```bash
./execfile
```

### Use a Build System (Optional)
If you are working on larger projects, you might want to use `make` or `cmake` to handle the build process. Install them with:
```bash
sudo apt install make cmake
```
You can then set up `Makefiles` or `CMakeLists.txt` for more complex projects.

### Debugging Tools (Optional)
For debugging, you can install `gdb`, the GNU debugger:
```bash
sudo apt install gdb
```

---

Day 20: Read about CUDA Programming 

---

## Day 19: Continuation with the training of the classifier

---

## Day 18: Continuation with the training of the classifier

---

## Day 17: Training a Classifier

---

## Day 16: Read through supervised learning from Andrew Ng notes

---

## Day 15: Last day on an intro to Distributed and Data Parallel programming

---

## Day 14: I researched and wrote an article on how to find and use GPUs in Pytorch

	 	 	 	  
Overcoming GPU Limitations in AI Research: Tips for Resource-Constrained Developers

Hello!

The race for compute power is a hot topic among AI Researchers. The race for compute power is a critical challenge in the world of AI and machine learning. NVIDIA, a $3 trillion company based in Santa Clara, California, is leading this revolution.

For many researchers—especially those in low-resource regions—access to GPUs is a major barrier. But don’t let that stop you!

In this post, I’ll cover the role of GPUs in machine learning, using them with PyTorch, leveraging multiple GPUs, and—most importantly—how to access GPUs without purchasing one, especially if you're from a low-resource region like me.

**So, why are GPUs essential in AI?**  
 GPUs handle the heavy math behind AI, processing vast datasets efficiently thanks to their parallelized cores. While your typical home computer has a CPU with a few cores, GPUs have thousands—making them perfect for training complex machine learning models.

**Using GPUs in PyTorch:**  
 You can easily check for available GPUs in PyTorch with this code:


```python
import torch

device = ("cuda"
          if torch.cuda.is\_available()
          else "mps"
          if torch.backends.mps.is_available()
          else "cpu"
)
print(f"Using {device} device")**

```

Checkout the simple explanation from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) on how to move your tensors to the GPU.

Want to scale up? You can leverage [**Distributed and Parallel Training**](https://pytorch.org/tutorials/distributed/home.html) using techniques like DistributedDataParallel (DDP) and Tensor Parallel (TP) to train large models on multiple GPUs.

**No GPU? No problem!**  
 Here are a few free alternatives to get started:

* **Kaggle Kernels**: 	Free access to Nvidia K80 GPUs. [Learn 	more](https://www.kaggle.com/code/dansbecker/running-kaggle-kernels-with-a-gpu).  
   	  
* **Google Colab**: Free GPUs for small projects, 	with Pro options for more power. [Read 	more](https://www.geeksforgeeks.org/how-to-use-gpu-in-google-colab/).

For larger projects, check [**Google Cloud**](https://cloud.google.com/compute/gpus-pricing?_gl=1*152ua1h*_up*MQ..&gclid=CjwKCAjw6JS3BhBAEiwAO9waF1gethq5sbpnf6Nb14gL8alrn4Tr4wr8F6ZAaKuXSiZAmPGRVfcoEhoCJlsQAvD_BwE&gclsrc=aw.ds) pricing or explore platforms like [**vast.ai**](https://vast.ai/) for affordable GPU rentals. Also, check out this blog on the [**Top 10 cloud GPU platforms for deep learning**](https://blog.paperspace.com/top-ten-cloud-gpu-platforms-for-deep-learning/) by Samuel Ozechi.

I hope this helps someone out there facing similar challenges. Keep pushing forward in your AI research, and good luck on your journey\! 💪🚀


---

## Day 13: A continuation with Distributed and Parallel Training Tutorials

> *Note* : *Fun Joke!* *What is the dictionary definition of shard?*
> - (online gaming) An instance of an MMORPG that is one of several independent and structurally identical virtual worlds, none of which has so many players as to exhaust a system's resources.
> - The other is, (database) A component of a sharded distributed database.
> - Synonyms: partition 

---

## Day 12: A continuation with Distributed and Parallel Training Tutorials

---

## Day 11: A continuation with Distributed and Parallel Training Tutorials


---

## Day 10: A continuation with Distributed and Parallel Training Tutorials


---

## Day 9: A peek into Distributed and Parallel Training Tutorials
Distributed training is a model training paradigm that involves spreading training workload across multiple worker nodes, therefore significantly improving the speed of training and model accuracy. While distributed training can be used for any type of ML model training, it is most beneficial to use it for large models and compute demanding tasks as deep learning.


---

## Day 8: More about torch utils data


---

## Day 7: Regularization 
I read about Regularization in machine learning.

---

## Day 6: GPUs
Today read about and studied about GPUs, how they work, their functionality and their role in training machine learning models.

---

## Day 5: Transforms

Data does not always come in its final processed form that is required for training machine learning algorithmms. We use `transforms` to perform some manipulation of the data and make it suitable for training.

All TorchVision datasets have two parameters -`transform` to modify the features and `target_transform` to modify the labels-that accept callables containing the transformation logic. The `torchvision.transforms` module offers serveral commonly-used transforms out of the box.

The FashionMNIST features are in PIL image format, and the labels are integers. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make these transformations, we use `ToTensors` and `Lambda`.

## Build the Neural Network
Neural networks comprise of layers/modules that perform operations on data. The `torch.nn` namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the `nn.Module`. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.

In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset.

---

## Day 4: Datasets and Dataloaders  

Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability. Pytorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. `Dataset` stores the samples and their corressponding labels, and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples.

PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass `torch.utils.data.Dataset` and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can be used to prototype and benchmark your model.

---

## Day 3: Deep Dive into Tensors and the `@` Operator

### The Importance of the `@` Operator in PyTorch
- **Matrix Multiplication:** The `@` operator simplifies matrix multiplication for 2D tensors, making your code more readable.
- **Matrix-Vector Multiplication:** Easily multiply a matrix by a vector using the `@` operator.
- **Dot Product:** Compute the dot product between two vectors with a single `@` operation.

This operator is a game-changer for making matrix operations more intuitive compared to `torch.matmul()`.

---

## Day 2: Continuing with Tensors and PyTorch Basics

### Understanding Tensors
Tensors are the backbone of PyTorch. They're similar to NumPy arrays but are optimized for GPU operations and automatic differentiation. Whether you're dealing with data inputs, outputs, or model parameters, tensors will be your go-to data structure.

- **Flexibility:** Tensors can seamlessly interact with NumPy arrays, often sharing memory without data copying.
- **Optimization:** Designed for GPU acceleration and automatic differentiation, making them ideal for deep learning.

---

## Day 1: Getting Started with PyTorch and FashionMNIST

### Quickstart with PyTorch and FashionMNIST
The first step in my journey was a basic introduction to PyTorch using the FashionMNIST dataset. This popular dataset, available via `torchvision`, is a great way to start experimenting with image classification.

- **Link to Torchvision Dataset:** [Click here](https://pytorch.org/vision/stable/datasets.html)

### Loading the Dataset
The `DataLoader` in PyTorch makes it easy to handle large datasets. By passing our dataset as an argument to `DataLoader`, we can efficiently batch, shuffle, and load data using multiple processes.

- **Batch Size:** In my example, I used a batch size of 64, which is a common choice for training deep learning models.

---

## Useful Tips and Commands

### How to Check for a Particular GPU

1. **Using the `lspci` Command:**
   - To list all PCI devices, including the GPU:
     ```bash
     lspci | grep -i vga
     ```
   - For more detailed information:
     ```bash
     lspci -v | grep -i vga
     ```

These commands are essential for confirming the presence and details of a GPU in your system, which is crucial when working with deep learning models that benefit from GPU acceleration.

---

## Goals and Expectations

This README will evolve as I progress through the 100 days, documenting my learning, challenges, and achievements. Whether you’re following along or just browsing, I hope you find this resource helpful!

---
