{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e540d48c-53ec-43ed-80ad-7b59e5647339",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c5f9cc-71e5-4eb1-8fe3-a0d1d1c32b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0661b702-9be2-40b3-a75a-819921d21c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.4.1 (from torchvision)\n",
      "  Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (70.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1->torchvision)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Collecting triton==3.0.0 (from torch==2.4.1->torchvision)\n",
      "  Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchvision) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->torch==2.4.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n",
      "Using cached torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
      "Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl (797.0 MB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
      "Installing collected packages: triton, nvidia-cudnn-cu12, torch, torchvision\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "Successfully installed nvidia-cudnn-cu12-9.1.0.70 torch-2.4.1 torchvision-0.19.1 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e92f6b-595a-494f-adf6-0b1a21bb4340",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.library' has no attribute 'register_fake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensor\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torchvision/_meta_registrations.py:163\u001b[0m\n\u001b[1;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         ),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_fake\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision::nms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_nms\u001b[39m(dets, scores, iou_threshold):\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should be a 2d tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should have 4 elements in dimension 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.library' has no attribute 'register_fake'"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5560b-4dc1-4468-a691-a63c214d8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ec299-4e78-4afd-bd5c-ef782fae163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fce78-1aa8-4942-b74a-9a00136dfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Models\n",
    "'''\n",
    "    To define a neural network in PyTorch, we create a class that inherits from the `nn.Module`. We define the layers of the network in the __init__ function and specify how data will pass through\n",
    "    the network in the forward function. To accelerate operations in the neural network, we move it to the GPU of MPS if available.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ce609-9633-4138-9d65-08a178a6ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6a5dc-1987-4c34-8dc5-5e13bd9ff63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f814899-c251-46a4-b8ed-6184d47680bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing the Model Parameters\n",
    "## To train a model, we need a loss function and an optimizer.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parametes(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4eac3-3fc4-4074-8732-c03634d2fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model's parameters.\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "    # Compute prediction error\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f} [{current:>5d} / {size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3488f-59d6-4f6f-9a20-a8a2404740c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also check the model's performance against the test dataset to ensure it is learning\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argumax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct) :> 0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55807c53-75fd-4325-9676-fb2767386025",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and \n",
    "    loss at each epoch; we'd like to see the accuracy increase and the loss decrease with every epoch.\n",
    "'''\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epochs {t+1}\\n---------------------------------------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8550ae9-31d9-4baf-8c8f-baf313dd5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Models\n",
    "# A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved Pytorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8ab9a-4fce-4ab2-bfe3-916f3153214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Models\n",
    "# The process for loading a model includes re-creating the model structure and loading the state dictionary into it.\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900a034-590f-4209-a600-562e24b4724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcdea8-8c9d-476a-9de5-beebc2136f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d317c-4e8d-462c-928a-750abe0f1b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f6e4d-5d49-460d-ae2a-a81e8f131766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8ca92c-3cfc-4503-8a1f-5321689072f3",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb590d-224b-4bc7-9918-9f51bf5ca7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7783a-59a9-4d74-877d-a9e759a8c9d5",
   "metadata": {},
   "source": [
    "## Initializing a Tensor\n",
    "Tensors can be initialized in various ways. Take a look at the following examples:\n",
    "\n",
    "**Directly from data**  \n",
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7f661-9cd3-4b18-b09a-f5a0c7aba7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308bf65-c9e4-4932-a3a1-e26e9e562db7",
   "metadata": {},
   "source": [
    "## From a NumPy array\n",
    "Tensors can be created from NumPy arrays (and vice versa -see Bridge with NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f2cc1-d62a-4513-a56e-9afb6d75933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa3c64-380f-4a2b-80ea-05ee23b58c7a",
   "metadata": {},
   "source": [
    "## From another tensor:\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f162be-2be5-44a0-8055-ecd44b57ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"One Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeedeea-6ffd-4204-9628-b16003612542",
   "metadata": {},
   "source": [
    "## With random or constant values:\n",
    "'shape' is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d96d3a-be00-47ab-9edf-a3faaa72a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaff14-7276-4e6d-8025-4a5104525949",
   "metadata": {},
   "source": [
    "# Attributes of a Tensor\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b277bd-7991-421c-9560-6bc428111892",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of the tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490eb6c3-a555-4a31-ae4a-78406914aded",
   "metadata": {},
   "source": [
    "# Operations on Tensors  \n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are described in the pytorch documentation.\n",
    "Each of these operations can be run on the GPU (at typically higher speeds than on a CPU).\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e56e5-46fd-4830-b10b-bccf580b623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e5b67-f997-442e-8758-c47aa64bc7c3",
   "metadata": {},
   "source": [
    "## Standard numpy-like indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd042fd-dcb7-4ce7-9727-4b8b9243483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebeb80f-4530-403d-9b63-ae498082fcb9",
   "metadata": {},
   "source": [
    "**Joining tensors** You can use `torch.cat` to concatenate a sequence of tensors along a given dimension. See also `torch.stack`, another tensor joining operator that\n",
    "is subtly different from `torch.cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4390a5f-a7bc-497c-8f5d-5fd54be227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48281135-bddc-4297-9575-e4745af9c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059346bc-b10e-43dd-91c9-7ccd001ddef8",
   "metadata": {},
   "source": [
    "## Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c4729-faa9-44ba-8db0-faeddaf2158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0539c-3a50-4334-913d-284e2a962dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3a18e-856f-4f10-abb2-979fce29e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a91ac5-8a7e-4363-b6e4-9655d85f8fe2",
   "metadata": {},
   "source": [
    "## **Single-element tensors** \n",
    "If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python\n",
    "numerical value using `item`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed856db-4f72-4634-9698-218f535edd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7703b-6f5b-4556-99d8-200a77e7a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = torch.ones(4, 4, dtype=torch.int8)\n",
    "print(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e2dfa-6eba-4ac8-b684-da3c9c7239a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with the single-element tensors\n",
    "tes_set = tes.sum()\n",
    "print(tes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbe398-6935-43ab-bcda-252b084e3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_set = tes_set.item()\n",
    "print(tes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924bc4d4-2833-48a2-a783-5dff10c27751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tes_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab08d6-c902-41ac-bce4-8f0150d5b216",
   "metadata": {},
   "source": [
    "## **In-place operations**  \n",
    "Operations that store the result into the operand are called in-place. They are denoted by a _sufix. For example: `x.copy_(y)`, `x.t_()`, will change `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b60b07-e7f4-48c8-b5e2-f1b5683a9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da705f-3411-4520-a599-340b94bd2a5e",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> In-place operations save some memory, but can be problematic when computing derivatives because of an immediate  loss of history. Hence, their use is discouraged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f99ce-c2c5-4ee1-b94b-6523eb2af9fd",
   "metadata": {},
   "source": [
    "## Bridge with NumPy\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2d913-42fb-47c2-9712-1f768d143ae5",
   "metadata": {},
   "source": [
    "## Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bfe73-6e1a-4f46-a0d5-d825dbec30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12849f-576c-49fa-9242-8ffacfc21e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A change in the tensor reflects in the NumPy array.\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5a179-0c52-48e5-836b-78efb6c0dbab",
   "metadata": {},
   "source": [
    "## NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b2b13-460b-433f-9fe0-74e5d20c7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525abf6d-482d-43bd-af21-31e06eee356e",
   "metadata": {},
   "source": [
    "**Changes in the NummPy array reflects in the tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25a13a-cecc-4eef-8283-16c9655ba058",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca132a-a1e2-48a4-aaf5-9ec559f2e2ff",
   "metadata": {},
   "source": [
    "## Practice tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65bbd8-55ef-423b-b114-5ef8cf9d17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tensors\n",
    "# From value\n",
    "val = 12\n",
    "tensor_from_value = torch.tensor(val)\n",
    "print(val)\n",
    "print(tensor_from_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4496c95-9f51-4dd0-a26c-93254912ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of ones\n",
    "tensor_of_ones = torch.ones(2, 2)\n",
    "print(tensor_of_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f443d74-8726-4ea9-8632-1bd5e4269d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of ones with datatype\n",
    "tensor_of_ones_v2 = torch.ones(2, 2, dtype=torch.int16)\n",
    "print(tensor_of_ones_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470e72a-b37c-4cdb-9469-fcb7d6466994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of random values between 0 and 1\n",
    "tensor_random = torch.rand(2, 3)\n",
    "print(tensor_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0cfa4-65d5-40fe-b4b8-53e06e4b66a8",
   "metadata": {},
   "source": [
    "# Datasets and DataLoaders  \n",
    "---\n",
    "\n",
    "## Loading a Dataset  \n",
    "Here is an example of how to load the **Fashion-MNIST** dataset from TorchVision. Fashion-MNIST is a dataset of Zalando's article images consisting of 60,000 training samples and 10,000 test samples. Each example comprises a 28x28 grayscale image and an associated label from one of 10 classes.\n",
    "\n",
    "We load the **FashionMNIST Dataset** with the following parameters:  \n",
    "* `root` is the path where the train/test data is stored,\n",
    "* `train` specifics training or test dataset,\n",
    "* `download=True` downloads the data from the internet if it's not available at `root`,\n",
    "* `transform` and `target_transform` specify the feature and the label transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ac9b8-d521-4648-b01d-9942181ad990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223d789-144d-4a55-8a91-3efe247e0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e26d21-0431-4082-8344-f7af6f31d23f",
   "metadata": {},
   "source": [
    "## Iterating and Visualizing the Dataset  \n",
    "We can index `Datasets` manually like a list: `training_data[index]`. We use matplotlib to visualize some samples in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89671d-1be8-4494-a72f-e19dae51ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trowser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplots(rows, cols, i)\n",
    "    plt.title(labels_map[lable])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdbff4-ce78-4a5a-95ce-08f12ba82221",
   "metadata": {},
   "source": [
    "## Creating a Custom Dataset for your files  \n",
    "A custom Dataset class must implement three functions: *__init__, __len__, and __getitem__*. Take a look at this implementations; the FashionMNIST images are stored in a directory `img_dir`, and their labels are stored separately in a CSV file `annotations_file`.  \n",
    "\n",
    "In the next sections, we'll breakdown what's happening in each of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4251e-f5e2-4428-bb45-32dbda57a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89007c4e-7afe-4863-99ad-d8ceea1ac7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cebc008-40d7-4dc4-a949-af46a8f18c20",
   "metadata": {},
   "source": [
    "## `__init__`\n",
    "The '__init__' function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms, this will be covered in more detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485c546-9807-49fc-a7b7-7ddeca646331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8c8f6-589d-4ed7-9bbd-d3d44eaa0d32",
   "metadata": {},
   "source": [
    "## `__len__`  \n",
    "The '__len__' function returns the number of samples in our dataset.  \n",
    "Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c99d57-436c-4657-8d91-c3f65b50cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6afa5-0aae-4ed3-a342-4d9f75cb6eb2",
   "metadata": {},
   "source": [
    "## `__getitem__`  \n",
    "The __getitem__ function loads and returns a sample from the dataset at the given `idx`. Based on the index, it identifies the image's location on disk, converts that to a tensor using the `read_image`, retrieves the corresponding label from the csv data in `self.img_labels`, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01cfc3-c7d8-411d-a2a0-711fc6c85a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = read_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1bba15-38cc-445d-9f77-65620ee29f07",
   "metadata": {},
   "source": [
    "## Preparing your data for training with DataLoaders\n",
    "\n",
    "The `Dataset` retrieves our dataset's features and labels one sample at a time. While training a model, we typically want to pass samples in \"minibatches\", reshuffle the data at every epoch to reduce model overfitting, and use Python's `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "`DataLoader` is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f85ee-ef1f-4a17-b119-b0946b6eb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f13e2-141c-4881-b7cb-41fae10efed3",
   "metadata": {},
   "source": [
    "## Iterate through the DataLoader  \n",
    "We have loaded that dataset into the `DataLoader` and can iterate through the dataset as needed. Each iteration below returns a batch of `train_features` and `train_labels` (containing `batch_size=64` features and labels respectively). Because we specified `shuffle=True`, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa76876-249e-4132-85aa-17767379459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d25a8-711a-4404-b675-a445e2c18d4b",
   "metadata": {},
   "source": [
    "## torch.utils.data\n",
    "At the heart of PyTorch data loading utility is the `torch.utils.data.DataLoader` class. It represents a Python iterable over a dataset, with support for:  \n",
    "- map-style and iterable-style datasets,\n",
    "- customizing data loading order,\n",
    "- automatic batching,\n",
    "- single and multi-process data loading,\n",
    "- automatic memory pinning.\n",
    "\n",
    "These options are configured by the constructor arguments of a `DataLoader`, which has signature\n",
    "\n",
    "```Python\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,  \n",
    "            batch_sampler=None, num_workers=0, collate_fn=None,  \n",
    "            pin_memory=False, drop_last=False, timeout=0,  \n",
    "            worker_init_fn=None, x, prefetch_factor=2,  \n",
    "            peristent_workers=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0070a1f-f5a4-4954-afd7-ac5e33267f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88768f0f-92ea-4da3-ad17-b8b457eb3b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558fcc3-8fd0-4230-8c6f-c270b40b35de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79ec88-3a69-4fef-9db4-92aaca59614f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2417af7-94e4-4f73-8072-1d6f67388018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f52ef0-a67d-4adf-868f-f61a4107f366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c5004-a72b-4938-98e8-ec06062e7b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36cf4e-74d4-4fd7-b5f5-5ce2a8311e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484bf07e-08bb-4745-901c-5789e7993cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6795fd0-d5da-4685-87e2-db2230205cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ee042-2756-49a0-83b8-378f02748f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d93dce-4350-41ec-93ff-e21c5302969f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190a214-58bc-4862-b361-eb1193adf394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6acbf9-195f-46d3-850b-cd1215475be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fd262-7ff3-4f26-9e10-486cb676c3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f4c19-c9f5-4487-a415-733a74decba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72ebc-5a92-41e6-9525-cfffffb65e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88cacd-e67b-49a7-8124-be5c6a44f747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04255bd-4a1b-4800-8715-50dc485ef2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961637b-af18-4521-a749-1353774367af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32373180-ea86-432a-b382-4d7dd4257e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f858dd5-fe6c-4956-b611-986d5a94136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7cf4b-5e7c-41c6-bec7-c27a6e0ffd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb61d7-b1a9-4d45-b289-22dd01f2891f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469786c-344f-44fa-a0cb-409f6815a2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc37dfa-2b1b-4090-9ab3-37425b29299e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea154fcc-b384-4fb5-bac2-0c861bc1755f",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec4323-c73b-4b28-8c6c-dbc670129c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10,\n",
    "                                                 dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822221f7-d8d3-4274-972b-aa630d252e7f",
   "metadata": {},
   "source": [
    "## ToTensor  \n",
    "`ToTensor` converts a PIL image or NumPy `ndarray` into a `FloatTensor`. and scales the image's pixel intensity values in the range[0., 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29302030-1bef-47a7-830e-91d210e1781c",
   "metadata": {},
   "source": [
    "## Lambda Transforms\n",
    "Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls `scatter_` which assigns a `value=1` on the index as given by the label `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4f8fd3-9325-4983-87f1-50d1a0da3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d0936-08ef-4c98-b549-5e4e11889bb0",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e6236-799f-4c65-becb-85238a3af96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526b723-cb3f-4681-81ae-e1cb34dfba39",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "We want to be able to train our model on a hardware accelerator like the GPU or MPS, if available. Let's check to see if `torch.cuda` or `torch.backends.mps` are available, otherwise we use the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2f0c7-e3a9-42a0-bd0a-8d916bbf4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b934b-769b-4927-87c3-ea0b0fcbdd32",
   "metadata": {},
   "source": [
    "## Define the class\n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations the operations on input data in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4a4e8-1dfe-4e73-b17c-99231c94018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cd410-af1a-41d9-91ba-c389b6c45c36",
   "metadata": {},
   "source": [
    "We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e9d55-dfe3-4472-a0c5-4a7facddc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfea50f-421a-4a62-81d5-fd71141fc0e1",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model's `forward`, along with some `background operations`. Do not call `model.forward()` directly!\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cdcbc-0e21-4de9-b057-63bc494659c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = mm.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffa5918-93aa-4a1f-9ae6-643c3af9edf2",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "Let's break down the layers of the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9375caf-0cf1-4987-8a8e-b8a7b1eeb7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54046f4-9fa7-4060-b183-ce08a622a393",
   "metadata": {},
   "source": [
    "## nn.Flatten\n",
    "We initialize the `nn.Flatten` layer to convert each 2D 28x28 image into a contagious array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5dea3b-22be-4cc4-ba65-1164839ad4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a09d9-1b48-4a34-8b89-d31910a720ed",
   "metadata": {},
   "source": [
    "## nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c6fc8-d47e-4ec8-9d40-9f624acf8044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2a8a3-1d94-4ff1-bffe-d03882b07f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6e49a-4b29-43f3-a0f9-6ee7a9f7af85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89a841-238e-45f8-aaed-981f465d29ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99ad36-3fe7-4660-9aa8-6323002797ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e97cdd3-c47d-45d4-8078-6dcb3f42b490",
   "metadata": {},
   "source": [
    "# Distributed and Parallel Training Tutorial\n",
    "There are a few ways you can perform distributed training in PyTorch with each method having their advantages in certain use cases:\n",
    "* DistributedDataParallel (DDP)\n",
    "* Fully Shared Data Parallel (FSDP)\n",
    "* Tensor Parallel (TP)\n",
    "* Device Mesh\n",
    "* Remote Procedure Call (RPC) distributed training\n",
    "* Custom Extensions\n",
    "\n",
    "## Getting started with Distributed Data Parallel\n",
    "**Prerequisities:**\n",
    "* PyTorch Distributed Overview\n",
    "* DistributedDataParallel API documents\n",
    "* DistributedDataParallel notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42285d1f-90a9-4d7d-8ba9-387d4c8ed8e4",
   "metadata": {},
   "source": [
    "## Pytorch Distributed Overview\n",
    "This is the overview page for the `torch.distributed` package. The goal of this page is to categorize documents into different topics and briefly describe each of them. If this is your first time building distributed training applications using PyTorch, it is recommended to use this document to navigate to the technology that can best serve your case.\n",
    "\n",
    "## Introduction\n",
    "The PyTorch Distributed library includes a collective of parallelism modules, a communications layer and infrastructure for lauching and debugging large training jobs.\n",
    "\n",
    "## Parallelism APIs\n",
    "These Parallelism Modules offer high -level functionality and compose with existing models:\n",
    "- Distributed Data-Parallel (DDP)\n",
    "- Fully Sharded Data-Parallel Training (FSDP)\n",
    "- Tensor Parallel (TP)\n",
    "- Pipeline Parallel (PP)\n",
    "\n",
    "## Sharding primitives\n",
    "`DTensor` and `DeviceMesh` are primitives used to build parallelism in terms of sharded or replicated tensors on N-dimensional process groups.\n",
    "* `DTensor` represents a tensor that is sharded and/or replicated, and communicates automatically to reshard tensors as needed by operations.\n",
    "* `DeviceMesh` abstracts the accelerator device communicators into a multi-dimensional array, which manages the underlying `ProcessGroup` instances for collective communications in multi-dimensional parallelisms. Try out `Device Mesh Recipe` to learn more.\n",
    "\n",
    "## Communications APIs\n",
    "The `PyTorch distributed communication layer (C10D)` offers both collective communication APIs (e.g., `all_reduce` and `all_gather`) and P2P communication APIs (e.g., `send` and `isend`), which are used under the hood in all of parallelism implementations. `Writing Distributed Applications with PyTorch` shows examples of using c10d communication APIs.\n",
    "\n",
    "## Launcher\n",
    "`torchrun` is a widely-used launcher script, which spawns processes on the local and remote machines for running distributed PyTorch programs.\n",
    "\n",
    "## Applying Parallelism To Scale Your Model\n",
    "Data Parallelism is a widely adopted single-program multiple-data training paradigm where the model is replicated on every model replica computes local gradients for a different set of input data samples, gradients are averaged within the data-parallel communicator group before each optimizer step.\n",
    "\n",
    "Model Parallelism techniques (or sharded Data Parallelism) are required when a model doesn't fit in GPU, and can be combined together to form multi-dimensional (N-D) parallelism techniques.\n",
    "\n",
    "When deciding what parallelism techniques to choose for your model, use these common guidelines:\n",
    "\n",
    "1. Use `DistributedDataParallel (DDP)`, if your model fits in a single GPU but you want to easily scale up training using multiple GPUs.\n",
    "   * Use `torchrun` to launch multiple pytorch processes if you are using more than one node.\n",
    "2. Use `FullyShardedDataParallel (FSDP) ` when your model cannot fit on one GPU.\n",
    "3. Use `Tensor Parallel (TP)` and/or `Pipeline Parallel (PP)` if you reach scaling limitations with FSDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300bb44-b65e-435d-b2b8-5118f85b44b4",
   "metadata": {},
   "source": [
    "# Getting Started with DeviceMesh\n",
    "\n",
    "Prerequisites:\n",
    "* [Distributed Communication Package](https://pytorch.org/docs/stable/distributed.html) - `torch.distributed`\n",
    "\n",
    "Setting up distributed communicators, i.e. NVIDIA Collective Communication Library (NCCL) communicators, for distributed training can pose a signifant challenge. For workloads where users need to compose different parallelisms, users would need to manually set up and manage NCCL communicators (for example, `processGroup`) For each parallism solution. This process could be complicated and susceptible to errors.\n",
    "`DeviceMesh` can simplify this process, making it more manageable and less prone to errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b9886-aecd-49ef-ae6e-02faa95c5032",
   "metadata": {},
   "source": [
    "## What is DeviceMesh\n",
    "`DeviceMesh` is a higher level abstraction that manages `ProcessGroup`. It allows users to effortlessly create inter-node and intra-node process groups without worrying about to set up ranks correctly for different sub process groups. Users can also easily manage the underlying process_groups/devices for multi-dimensional parallelism via `DeviceMesh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c16ea-6c5f-4c55-9321-2a99ad3e8a8a",
   "metadata": {},
   "source": [
    "## Why DeviceMesh is Useful\n",
    "DeviceMesh is useful when working with multi-dimensional parallelism (i.e. 3-D parallel) where parallelism composability is required. For example, when your parallelism solutions require both communication across hosts and within each host. The image above shows that we can create a 2D mesh that connects the devices within each host, and connects each device with its counterpart on the other hosts in a homogenous setup\n",
    "\n",
    "Without DeviceMesh, users would need to manually set up NCCL communicators, cuda devices on each process before applying any parallelism, which could be quite complicated. The following code snippet illustrates a hybrid sharding 2-D Parallel pattern setup without `DeviceMesh`. First, we need to manually calculate the shard group and replicate group. Then, we need to assign the correct shard and replica group to each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3818687-21c2-4492-9de3-bd5bb46e0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41e14c5-2464-40b6-8c33-f5c6a7759534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand world topology\n",
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "print(f\"Running example on {rank=} in a world with {world_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1301faa-12a5-4d8f-954d-626bbdc87f92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create process groups to manage 2-D like parallel pattern\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdist\u001b[49m\u001b[38;5;241m.\u001b[39minit_process_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(rank)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dist' is not defined"
     ]
    }
   ],
   "source": [
    "# Create process groups to manage 2-D like parallel pattern\n",
    "dist.init_process_group(\"nccl\")\n",
    "torch.cuda.set_device(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674920a7-acfd-4c00-976c-faf0fc6ed2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create shard groups (e.g. (0, 1, 2, 3), (4, 5, 6, 7))\n",
    "# and assign the correct shard group to each rank\n",
    "num_node_devices = torch.cuda.device_count()\n",
    "shard_rank_lists = list(range(0, num_node_devices // 2)),\n",
    "list(range(num_node_devices // 2, num_node_devices))\n",
    "shard_groups = (\n",
    "    dist.new_group(shard_rank_lists[0]),\n",
    "    dist.new_group(shard_rank_lists[1]),\n",
    ")\n",
    "\n",
    "# Create replicate groups (for example, (0, 4), (1, 5), (2, 6), (3, 7))\n",
    "# and assign the correct replica group to each rank\n",
    "current_replicate_group = None\n",
    "shard_factor = len(shard_rank_lists[0])\n",
    "for i in range(num_node_devices // 2):\n",
    "    replica_group_ranks = list(range(i, num_node_devices, shard_factor))\n",
    "    replica_group = dist.new_group(replica_group_ranks)\n",
    "    if rank in replicate_group_ranks:\n",
    "        current_replicate_group = replicate_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd4e59-0090-4e36-8e8c-4ad996512ee8",
   "metadata": {},
   "source": [
    "To run the above code snippet, we can leverage PyTorch Elastic. Lets create a file named 2d_setup.py. Then, run the following torch elastic/torchrun command.\n",
    "\n",
    "`torchrun --nproc_per_node=8 --rdzu_id=100 --rdzu_endpoint=localhost:29400 2d_setup.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b584ae-880f-411f-a7f8-cebd635e70f8",
   "metadata": {},
   "source": [
    "Let's create a file named `2d_setup_with_device_mesh.py`. Then, run the following `torch elastic/torchrun` command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5acfb-2793-48ba-a073-179152b62180",
   "metadata": {},
   "source": [
    "```bash\n",
    "torchrun --nproc_per_node=8 2d_setup_with_device_mesh.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bfd487-c878-414d-841a-0cef8bbaaf87",
   "metadata": {},
   "source": [
    "## How to use DeviceMesh with HSDP\n",
    "Hybrid Sharding Data Parallel(HSDP) is 2D strategy to perform FSDP within a host and DDP across hosts.\n",
    "\n",
    "Let's see an example of how DeviceMesh can assist with applying HSDP to your model with a simple setup. With DeviceMesh, users would not need to manually create and manage shard group and replicate group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58021af8-7f81-4d88-b25e-9ead074c2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71228b7d-59ec-48ad-b4b4-fa73c24991ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.RELU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "# HSDP: MeshShape(2, 4)\n",
    "mesh_2d = init_device_mesh(\"cuda\", (2, 4))\n",
    "model = FSDP(\n",
    "    ToyModel(), device_mesh=mesh_2d,\n",
    "    sharding_strategy=ShardingStrategy.HYBRID_SHARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dfb913-4789-42c7-b1af-38f93d8147ce",
   "metadata": {},
   "source": [
    "Let's create a file named `hsdp.py`. Then, run the following `torch elastic/torchrun` command.\n",
    "\n",
    "```bash\n",
    "torchrun --nproc_per_node=8 hsdp.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701bb88f-727b-48ea-b380-b4838405ffc3",
   "metadata": {},
   "source": [
    "## How to use DeviceMesh for your custom parallel solutions\n",
    "\n",
    "When working with large scale training, you might have more complex custom parallel training composition. For example, you may need to slice out submeshes for different parallelism solutions. DeviceMesh allows users to slice child mesh from the parent mesh and re-use the NCCL communicators already created when the parent mesh is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86643159-a312-41be-857f-27b5a08c46a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2d6f06-d63c-4af4-95ce-d6f25eee562d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79986752-30c4-4916-9794-36d275af6ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6513ba-5ac1-48f2-ad4c-3702754100ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e4b08-8de2-4ec5-8eb6-db69a5a590d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bea20e1-c812-42f8-a424-a4ee92427700",
   "metadata": {},
   "source": [
    "# Training a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cafcb9-ba94-445d-be90-b760cea39662",
   "metadata": {},
   "source": [
    "## Dataset used: `cifar10`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ccc9ea-39b5-4709-bfd9-d6cb5f08733a",
   "metadata": {},
   "source": [
    "## We will do the following steps in order\n",
    "1. Load and normalize the CIFAR10 training and test datasets using `torchvision`\n",
    "2. Define a Convolution Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafd532-ff9d-43fb-adad-b48bd4969f9f",
   "metadata": {},
   "source": [
    "## 1. Load and normalize CIFAR10\n",
    "Using `torchvision`, it's extremely easy to load CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f3fbf8-0cf4-4518-b8ab-3fd3bab4d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9dbf7-a24b-46d0-89cf-81a40c4a4de6",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range \\[0, 1]. We transform them to Tensors of normalized range \\[-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f064f43c-d801-4678-8988-cde7fbf4f797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 170498071/170498071 [02:40<00:00, 1063619.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "          'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43ef05-fc05-44cf-a544-e32471cc8950",
   "metadata": {},
   "source": [
    "Let us show some of the training images for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf3bc2c7-d352-4f98-8c21-3418b854b12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truck frog  dog   cat  \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show as image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5    # unnormalize\n",
    "    nping = img.numpy()\n",
    "    plt.imshow(np.transpose(nping, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "detaiter = iter(trainloader)\n",
    "images, labels = next(detaiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56622460-030d-4efa-90cf-b818b13d7368",
   "metadata": {},
   "source": [
    "## 2. Define a Convolutional Neural Network\n",
    "Copy the neural network from the Neural Networks section before and modify it to take 3-channel images (instead of 1-channel images as it was defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d67493ff-a9cf-4ce5-8399-978bb2f23c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3247e41c-931e-4030-aac0-cbbc2f52969c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb00aeb-96f9-4866-8f1b-ef4951085767",
   "metadata": {},
   "source": [
    "# Define a Loss function and optimizer\n",
    "Let's use a Classification Cross-Entropy loss and SGD with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97035fb1-ec99-4f04-bfe6-4108d4a477cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f720e1-2733-43c4-82f1-867342e345ac",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "This is when things start to get interesting. We simply have to loop over our data iterator, and feed the inputs to the network and optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1daee7a0-3624-4a79-a2e4-e21457f5097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.212\n",
      "[1,  4000] loss: 1.888\n",
      "[1,  6000] loss: 1.684\n",
      "[1,  8000] loss: 1.582\n",
      "[1, 10000] loss: 1.527\n",
      "[1, 12000] loss: 1.463\n",
      "[2,  2000] loss: 1.391\n",
      "[2,  4000] loss: 1.357\n",
      "[2,  6000] loss: 1.331\n",
      "[2,  8000] loss: 1.323\n",
      "[2, 10000] loss: 1.299\n",
      "[2, 12000] loss: 1.265\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):   # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3479171-0153-46e2-b8bc-d8f1bc539b86",
   "metadata": {},
   "source": [
    "Let's quickly save our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beb44aca-f08e-4f14-bcae-bcb3dcc8c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.path'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bcdf4b-3921-4c15-91dd-de7538d25de5",
   "metadata": {},
   "source": [
    "## 5. Test the network on the test data\n",
    "We have trained the network for 2 passes over the training dataset. But we need to check if the network has learnt anything at all.\n",
    "\n",
    "We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Okay, first step. Let us display an image from the test set to get familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68d90cf0-a46e-4abf-a50d-0966fb65fdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  truck bird  horse ship \n"
     ]
    }
   ],
   "source": [
    "detailer = iter(testloader)\n",
    "images, labels = next(detaiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4a610-9ef7-4335-b1dd-5d7f3e944b99",
   "metadata": {},
   "source": [
    "Next, let's load back in our saved model (note: saving and re-loading the model wasn't necessary here, we only did it to illustrate how to do so):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8954158-8e0a-42a9-9b9d-02e157c50752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf76d99-0781-496f-859d-998166c58fe4",
   "metadata": {},
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0145a2fd-60d4-43fd-bbcc-9d16618de070",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697a307-2b78-40da-ac25-653ba24e9fa6",
   "metadata": {},
   "source": [
    "The outputs are energies for the 10 classes. The higher the energy for a class, the more the network thinks that the image is of the particular class. So, let's get the index of the highest energy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8638e737-57c2-478f-8da3-5832b7f9e2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  bird  bird  horse plane\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                                for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150eab2-ad98-4134-9852-c9cd85c158c8",
   "metadata": {},
   "source": [
    "The results seem pretty good.\n",
    "\n",
    "Let us look at how the network performs on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f8465df-8b1b-4762-afa0-59fa7d999c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 55%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8001cbb8-66a9-4e39-a76c-27392303378b",
   "metadata": {},
   "source": [
    "That looks way better than chance, which is 10% accuracy (randomly picking a class out of 10 classes). Seems like the network learnt something.\n",
    "\n",
    "Hmmm, what are the classes that performed well, and the classes that did not perform well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70e9e71a-5d4d-442b-b605-55126609f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: plane is 64.8 %\n",
      "Accuracy for class: car   is 72.4 %\n",
      "Accuracy for class: bird  is 43.2 %\n",
      "Accuracy for class: cat   is 42.6 %\n",
      "Accuracy for class: deer  is 47.1 %\n",
      "Accuracy for class: dog   is 39.4 %\n",
      "Accuracy for class: frog  is 76.2 %\n",
      "Accuracy for class: horse is 55.6 %\n",
      "Accuracy for class: ship  is 66.1 %\n",
      "Accuracy for class: truck is 51.4 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c906fe1-726f-4ce9-8ceb-46ca764ea0da",
   "metadata": {},
   "source": [
    "Okay, so what is next?\n",
    "\n",
    "How do we run these neural networks on the GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c05e8-1c90-441c-b8c1-85dd41c94823",
   "metadata": {},
   "source": [
    "## Training on GPU\n",
    "Just like how you transfer onto the GPU, you transfer the neural net onto the GPU.\n",
    "Let's first define our device as the first visible cuda device if we have CUDA available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50d6be21-242a-426e-9157-498e1f15437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072458e-a610-4cbb-810e-e17828be10d1",
   "metadata": {},
   "source": [
    "The rest of this section assumes that `device` is a CUDA device\n",
    "Then these methods will recursively go over all modules and covert their parameters and buffers to CUDA tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e507613-35cb-4d1c-8aea-90d28dc4e3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52648f-4563-45ce-8a8c-039d54f144d1",
   "metadata": {},
   "source": [
    "Remember that you will have to send the inputd and targets at every step to the GPU too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b4f09c-e9f2-4397-8633-cccbc8aaafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = data[0].to(device), data[1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005dc25-e871-4cb8-a1d2-fa7571d67652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
