{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e540d48c-53ec-43ed-80ad-7b59e5647339",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c5f9cc-71e5-4eb1-8fe3-a0d1d1c32b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0661b702-9be2-40b3-a75a-819921d21c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.4.0 (from torchvision)\n",
      "  Using cached torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (70.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->torchvision)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.0->torchvision) (12.1.105)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0->torchvision)\n",
      "  Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->torchvision) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->torch==2.4.0->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n",
      "Using cached torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
      "Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/797.2 MB\u001b[0m \u001b[31m14.0 kB/s\u001b[0m eta \u001b[36m14:02:19\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/http/client.py\", line 479, in read\n",
      "    s = self.fp.read(amt)\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/ssl.py\", line 1252, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/ssl.py\", line 1104, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_internal/cli/base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_internal/cli/req_command.py\", line 245, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_internal/commands/install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 552, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_internal/operations/prepare.py\", line 467, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_internal/network/download.py\", line 183, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e92f6b-595a-494f-adf6-0b1a21bb4340",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5560b-4dc1-4468-a691-a63c214d8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ec299-4e78-4afd-bd5c-ef782fae163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fce78-1aa8-4942-b74a-9a00136dfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Models\n",
    "'''\n",
    "    To define a neural network in PyTorch, we create a class that inherits from the `nn.Module`. We define the layers of the network in the __init__ function and specify how data will pass through\n",
    "    the network in the forward function. To accelerate operations in the neural network, we move it to the GPU of MPS if available.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ce609-9633-4138-9d65-08a178a6ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6a5dc-1987-4c34-8dc5-5e13bd9ff63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f814899-c251-46a4-b8ed-6184d47680bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing the Model Parameters\n",
    "## To train a model, we need a loss function and an optimizer.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parametes(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4eac3-3fc4-4074-8732-c03634d2fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model's parameters.\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "    # Compute prediction error\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f} [{current:>5d} / {size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3488f-59d6-4f6f-9a20-a8a2404740c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also check the model's performance against the test dataset to ensure it is learning\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argumax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct) :> 0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55807c53-75fd-4325-9676-fb2767386025",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and \n",
    "    loss at each epoch; we'd like to see the accuracy increase and the loss decrease with every epoch.\n",
    "'''\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epochs {t+1}\\n---------------------------------------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8550ae9-31d9-4baf-8c8f-baf313dd5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Models\n",
    "# A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved Pytorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8ab9a-4fce-4ab2-bfe3-916f3153214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Models\n",
    "# The process for loading a model includes re-creating the model structure and loading the state dictionary into it.\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900a034-590f-4209-a600-562e24b4724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcdea8-8c9d-476a-9de5-beebc2136f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d317c-4e8d-462c-928a-750abe0f1b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f6e4d-5d49-460d-ae2a-a81e8f131766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8ca92c-3cfc-4503-8a1f-5321689072f3",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb590d-224b-4bc7-9918-9f51bf5ca7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7783a-59a9-4d74-877d-a9e759a8c9d5",
   "metadata": {},
   "source": [
    "## Initializing a Tensor\n",
    "Tensors can be initialized in various ways. Take a look at the following examples:\n",
    "\n",
    "**Directly from data**  \n",
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7f661-9cd3-4b18-b09a-f5a0c7aba7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308bf65-c9e4-4932-a3a1-e26e9e562db7",
   "metadata": {},
   "source": [
    "## From a NumPy array\n",
    "Tensors can be created from NumPy arrays (and vice versa -see Bridge with NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f2cc1-d62a-4513-a56e-9afb6d75933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa3c64-380f-4a2b-80ea-05ee23b58c7a",
   "metadata": {},
   "source": [
    "## From another tensor:\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f162be-2be5-44a0-8055-ecd44b57ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"One Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeedeea-6ffd-4204-9628-b16003612542",
   "metadata": {},
   "source": [
    "## With random or constant values:\n",
    "'shape' is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d96d3a-be00-47ab-9edf-a3faaa72a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaff14-7276-4e6d-8025-4a5104525949",
   "metadata": {},
   "source": [
    "# Attributes of a Tensor\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b277bd-7991-421c-9560-6bc428111892",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of the tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490eb6c3-a555-4a31-ae4a-78406914aded",
   "metadata": {},
   "source": [
    "# Operations on Tensors  \n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are described in the pytorch documentation.\n",
    "Each of these operations can be run on the GPU (at typically higher speeds than on a CPU).\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e56e5-46fd-4830-b10b-bccf580b623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e5b67-f997-442e-8758-c47aa64bc7c3",
   "metadata": {},
   "source": [
    "## Standard numpy-like indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd042fd-dcb7-4ce7-9727-4b8b9243483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebeb80f-4530-403d-9b63-ae498082fcb9",
   "metadata": {},
   "source": [
    "**Joining tensors** You can use `torch.cat` to concatenate a sequence of tensors along a given dimension. See also `torch.stack`, another tensor joining operator that\n",
    "is subtly different from `torch.cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4390a5f-a7bc-497c-8f5d-5fd54be227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48281135-bddc-4297-9575-e4745af9c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059346bc-b10e-43dd-91c9-7ccd001ddef8",
   "metadata": {},
   "source": [
    "## Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c4729-faa9-44ba-8db0-faeddaf2158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0539c-3a50-4334-913d-284e2a962dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3a18e-856f-4f10-abb2-979fce29e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a91ac5-8a7e-4363-b6e4-9655d85f8fe2",
   "metadata": {},
   "source": [
    "## **Single-element tensors** \n",
    "If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python\n",
    "numerical value using `item`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed856db-4f72-4634-9698-218f535edd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7703b-6f5b-4556-99d8-200a77e7a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = torch.ones(4, 4, dtype=torch.int8)\n",
    "print(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e2dfa-6eba-4ac8-b684-da3c9c7239a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with the single-element tensors\n",
    "tes_set = tes.sum()\n",
    "print(tes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbe398-6935-43ab-bcda-252b084e3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_set = tes_set.item()\n",
    "print(tes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924bc4d4-2833-48a2-a783-5dff10c27751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tes_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab08d6-c902-41ac-bce4-8f0150d5b216",
   "metadata": {},
   "source": [
    "## **In-place operations**  \n",
    "Operations that store the result into the operand are called in-place. They are denoted by a _sufix. For example: `x.copy_(y)`, `x.t_()`, will change `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b60b07-e7f4-48c8-b5e2-f1b5683a9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da705f-3411-4520-a599-340b94bd2a5e",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> In-place operations save some memory, but can be problematic when computing derivatives because of an immediate  loss of history. Hence, their use is discouraged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f99ce-c2c5-4ee1-b94b-6523eb2af9fd",
   "metadata": {},
   "source": [
    "## Bridge with NumPy\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2d913-42fb-47c2-9712-1f768d143ae5",
   "metadata": {},
   "source": [
    "## Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bfe73-6e1a-4f46-a0d5-d825dbec30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12849f-576c-49fa-9242-8ffacfc21e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A change in the tensor reflects in the NumPy array.\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5a179-0c52-48e5-836b-78efb6c0dbab",
   "metadata": {},
   "source": [
    "## NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b2b13-460b-433f-9fe0-74e5d20c7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525abf6d-482d-43bd-af21-31e06eee356e",
   "metadata": {},
   "source": [
    "**Changes in the NummPy array reflects in the tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25a13a-cecc-4eef-8283-16c9655ba058",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca132a-a1e2-48a4-aaf5-9ec559f2e2ff",
   "metadata": {},
   "source": [
    "## Practice tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65bbd8-55ef-423b-b114-5ef8cf9d17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tensors\n",
    "# From value\n",
    "val = 12\n",
    "tensor_from_value = torch.tensor(val)\n",
    "print(val)\n",
    "print(tensor_from_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4496c95-9f51-4dd0-a26c-93254912ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of ones\n",
    "tensor_of_ones = torch.ones(2, 2)\n",
    "print(tensor_of_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f443d74-8726-4ea9-8632-1bd5e4269d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of ones with datatype\n",
    "tensor_of_ones_v2 = torch.ones(2, 2, dtype=torch.int16)\n",
    "print(tensor_of_ones_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470e72a-b37c-4cdb-9469-fcb7d6466994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of random values between 0 and 1\n",
    "tensor_random = torch.rand(2, 3)\n",
    "print(tensor_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0cfa4-65d5-40fe-b4b8-53e06e4b66a8",
   "metadata": {},
   "source": [
    "# Datasets and DataLoaders  \n",
    "---\n",
    "\n",
    "## Loading a Dataset  \n",
    "Here is an example of how to load the **Fashion-MNIST** dataset from TorchVision. Fashion-MNIST is a dataset of Zalando's article images consisting of 60,000 training samples and 10,000 test samples. Each example comprises a 28x28 grayscale image and an associated label from one of 10 classes.\n",
    "\n",
    "We load the **FashionMNIST Dataset** with the following parameters:  \n",
    "* `root` is the path where the train/test data is stored,\n",
    "* `train` specifics training or test dataset,\n",
    "* `download=True` downloads the data from the internet if it's not available at `root`,\n",
    "* `transform` and `target_transform` specify the feature and the label transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ac9b8-d521-4648-b01d-9942181ad990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223d789-144d-4a55-8a91-3efe247e0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e26d21-0431-4082-8344-f7af6f31d23f",
   "metadata": {},
   "source": [
    "## Iterating and Visualizing the Dataset  \n",
    "We can index `Datasets` manually like a list: `training_data[index]`. We use matplotlib to visualize some samples in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89671d-1be8-4494-a72f-e19dae51ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trowser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplots(rows, cols, i)\n",
    "    plt.title(labels_map[lable])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdbff4-ce78-4a5a-95ce-08f12ba82221",
   "metadata": {},
   "source": [
    "## Creating a Custom Dataset for your files  \n",
    "A custom Dataset class must implement three functions: *__init__, __len__, and __getitem__*. Take a look at this implementations; the FashionMNIST images are stored in a directory `img_dir`, and their labels are stored separately in a CSV file `annotations_file`.  \n",
    "\n",
    "In the next sections, we'll breakdown what's happening in each of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4251e-f5e2-4428-bb45-32dbda57a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89007c4e-7afe-4863-99ad-d8ceea1ac7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cebc008-40d7-4dc4-a949-af46a8f18c20",
   "metadata": {},
   "source": [
    "## `__init__`\n",
    "The '__init__' function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms, this will be covered in more detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485c546-9807-49fc-a7b7-7ddeca646331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8c8f6-589d-4ed7-9bbd-d3d44eaa0d32",
   "metadata": {},
   "source": [
    "## `__len__`  \n",
    "The '__len__' function returns the number of samples in our dataset.  \n",
    "Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c99d57-436c-4657-8d91-c3f65b50cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6afa5-0aae-4ed3-a342-4d9f75cb6eb2",
   "metadata": {},
   "source": [
    "## `__getitem__`  \n",
    "The __getitem__ function loads and returns a sample from the dataset at the given `idx`. Based on the index, it identifies the image's location on disk, converts that to a tensor using the `read_image`, retrieves the corresponding label from the csv data in `self.img_labels`, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01cfc3-c7d8-411d-a2a0-711fc6c85a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = read_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1bba15-38cc-445d-9f77-65620ee29f07",
   "metadata": {},
   "source": [
    "## Preparing your data for training with DataLoaders\n",
    "\n",
    "The `Dataset` retrieves our dataset's features and labels one sample at a time. While training a model, we typically want to pass samples in \"minibatches\", reshuffle the data at every epoch to reduce model overfitting, and use Python's `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "`DataLoader` is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f85ee-ef1f-4a17-b119-b0946b6eb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f13e2-141c-4881-b7cb-41fae10efed3",
   "metadata": {},
   "source": [
    "## Iterate through the DataLoader  \n",
    "We have loaded that dataset into the `DataLoader` and can iterate through the dataset as needed. Each iteration below returns a batch of `train_features` and `train_labels` (containing `batch_size=64` features and labels respectively). Because we specified `shuffle=True`, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa76876-249e-4132-85aa-17767379459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d25a8-711a-4404-b675-a445e2c18d4b",
   "metadata": {},
   "source": [
    "## torch.utils.data\n",
    "At the heart of PyTorch data loading utility is the `torch.utils.data.DataLoader` class. It represents a Python iterable over a dataset, with support for:  \n",
    "- map-style and iterable-style datasets,\n",
    "- customizing data loading order,\n",
    "- automatic batching,\n",
    "- single and multi-process data loading,\n",
    "- automatic memory pinning.\n",
    "\n",
    "These options are configured by the constructor arguments of a `DataLoader`, which has signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0070a1f-f5a4-4954-afd7-ac5e33267f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88768f0f-92ea-4da3-ad17-b8b457eb3b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558fcc3-8fd0-4230-8c6f-c270b40b35de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79ec88-3a69-4fef-9db4-92aaca59614f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2417af7-94e4-4f73-8072-1d6f67388018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f52ef0-a67d-4adf-868f-f61a4107f366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c5004-a72b-4938-98e8-ec06062e7b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36cf4e-74d4-4fd7-b5f5-5ce2a8311e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484bf07e-08bb-4745-901c-5789e7993cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6795fd0-d5da-4685-87e2-db2230205cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ee042-2756-49a0-83b8-378f02748f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d93dce-4350-41ec-93ff-e21c5302969f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190a214-58bc-4862-b361-eb1193adf394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6acbf9-195f-46d3-850b-cd1215475be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fd262-7ff3-4f26-9e10-486cb676c3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f4c19-c9f5-4487-a415-733a74decba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72ebc-5a92-41e6-9525-cfffffb65e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88cacd-e67b-49a7-8124-be5c6a44f747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04255bd-4a1b-4800-8715-50dc485ef2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961637b-af18-4521-a749-1353774367af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32373180-ea86-432a-b382-4d7dd4257e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f858dd5-fe6c-4956-b611-986d5a94136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7cf4b-5e7c-41c6-bec7-c27a6e0ffd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb61d7-b1a9-4d45-b289-22dd01f2891f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469786c-344f-44fa-a0cb-409f6815a2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc37dfa-2b1b-4090-9ab3-37425b29299e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea154fcc-b384-4fb5-bac2-0c861bc1755f",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec4323-c73b-4b28-8c6c-dbc670129c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10,\n",
    "                                                 dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822221f7-d8d3-4274-972b-aa630d252e7f",
   "metadata": {},
   "source": [
    "## ToTensor  \n",
    "`ToTensor` converts a PIL image or NumPy `ndarray` into a `FloatTensor`. and scales the image's pixel intensity values in the range[0., 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29302030-1bef-47a7-830e-91d210e1781c",
   "metadata": {},
   "source": [
    "## Lambda Transforms\n",
    "Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls `scatter_` which assigns a `value=1` on the index as given by the label `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4f8fd3-9325-4983-87f1-50d1a0da3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d0936-08ef-4c98-b549-5e4e11889bb0",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e6236-799f-4c65-becb-85238a3af96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526b723-cb3f-4681-81ae-e1cb34dfba39",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "We want to be able to train our model on a hardware accelerator like the GPU or MPS, if available. Let's check to see if `torch.cuda` or `torch.backends.mps` are available, otherwise we use the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2f0c7-e3a9-42a0-bd0a-8d916bbf4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b934b-769b-4927-87c3-ea0b0fcbdd32",
   "metadata": {},
   "source": [
    "## Define the class\n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations the operations on input data in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4a4e8-1dfe-4e73-b17c-99231c94018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cd410-af1a-41d9-91ba-c389b6c45c36",
   "metadata": {},
   "source": [
    "We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e9d55-dfe3-4472-a0c5-4a7facddc63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466dff1-7d79-4fbc-9499-ce4f38216b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cdcbc-0e21-4de9-b057-63bc494659c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80405e-5f6a-43c8-851e-2e7738a0e5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9375caf-0cf1-4987-8a8e-b8a7b1eeb7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d917a9f-5bec-47b6-996e-3fba2692bad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
