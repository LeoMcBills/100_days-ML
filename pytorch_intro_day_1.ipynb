{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e540d48c-53ec-43ed-80ad-7b59e5647339",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c5f9cc-71e5-4eb1-8fe3-a0d1d1c32b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0661b702-9be2-40b3-a75a-819921d21c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.4.1 (from torchvision)\n",
      "  Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (70.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1->torchvision)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from torch==2.4.1->torchvision) (12.1.105)\n",
      "Collecting triton==3.0.0 (from torch==2.4.1->torchvision)\n",
      "  Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchvision) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->torch==2.4.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/leo/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n",
      "Using cached torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
      "Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl (797.0 MB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
      "Installing collected packages: triton, nvidia-cudnn-cu12, torch, torchvision\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "Successfully installed nvidia-cudnn-cu12-9.1.0.70 torch-2.4.1 torchvision-0.19.1 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e92f6b-595a-494f-adf6-0b1a21bb4340",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.library' has no attribute 'register_fake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensor\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torchvision/_meta_registrations.py:163\u001b[0m\n\u001b[1;32m    153\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(\n\u001b[1;32m    154\u001b[0m         grad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m rois\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         ),\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mnew_empty((batch_size, channels, height, width))\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_fake\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision::nms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_nms\u001b[39m(dets, scores, iou_threshold):\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should be a 2d tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_check(dets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes should have 4 elements in dimension 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.library' has no attribute 'register_fake'"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5560b-4dc1-4468-a691-a63c214d8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ec299-4e78-4afd-bd5c-ef782fae163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fce78-1aa8-4942-b74a-9a00136dfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Models\n",
    "'''\n",
    "    To define a neural network in PyTorch, we create a class that inherits from the `nn.Module`. We define the layers of the network in the __init__ function and specify how data will pass through\n",
    "    the network in the forward function. To accelerate operations in the neural network, we move it to the GPU of MPS if available.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ce609-9633-4138-9d65-08a178a6ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6a5dc-1987-4c34-8dc5-5e13bd9ff63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f814899-c251-46a4-b8ed-6184d47680bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing the Model Parameters\n",
    "## To train a model, we need a loss function and an optimizer.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parametes(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4eac3-3fc4-4074-8732-c03634d2fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model's parameters.\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "    # Compute prediction error\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "        loss, current = loss.item(), (batch + 1) * len(X)\n",
    "        print(f\"loss: {loss:>7f} [{current:>5d} / {size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3488f-59d6-4f6f-9a20-a8a2404740c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also check the model's performance against the test dataset to ensure it is learning\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argumax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct) :> 0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55807c53-75fd-4325-9676-fb2767386025",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    The training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model's accuracy and \n",
    "    loss at each epoch; we'd like to see the accuracy increase and the loss decrease with every epoch.\n",
    "'''\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epochs {t+1}\\n---------------------------------------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8550ae9-31d9-4baf-8c8f-baf313dd5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Models\n",
    "# A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved Pytorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8ab9a-4fce-4ab2-bfe3-916f3153214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Models\n",
    "# The process for loading a model includes re-creating the model structure and loading the state dictionary into it.\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900a034-590f-4209-a600-562e24b4724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcdea8-8c9d-476a-9de5-beebc2136f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d317c-4e8d-462c-928a-750abe0f1b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f6e4d-5d49-460d-ae2a-a81e8f131766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8ca92c-3cfc-4503-8a1f-5321689072f3",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb590d-224b-4bc7-9918-9f51bf5ca7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7783a-59a9-4d74-877d-a9e759a8c9d5",
   "metadata": {},
   "source": [
    "## Initializing a Tensor\n",
    "Tensors can be initialized in various ways. Take a look at the following examples:\n",
    "\n",
    "**Directly from data**  \n",
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7f661-9cd3-4b18-b09a-f5a0c7aba7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308bf65-c9e4-4932-a3a1-e26e9e562db7",
   "metadata": {},
   "source": [
    "## From a NumPy array\n",
    "Tensors can be created from NumPy arrays (and vice versa -see Bridge with NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f2cc1-d62a-4513-a56e-9afb6d75933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa3c64-380f-4a2b-80ea-05ee23b58c7a",
   "metadata": {},
   "source": [
    "## From another tensor:\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f162be-2be5-44a0-8055-ecd44b57ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"One Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeedeea-6ffd-4204-9628-b16003612542",
   "metadata": {},
   "source": [
    "## With random or constant values:\n",
    "'shape' is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d96d3a-be00-47ab-9edf-a3faaa72a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaff14-7276-4e6d-8025-4a5104525949",
   "metadata": {},
   "source": [
    "# Attributes of a Tensor\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b277bd-7991-421c-9560-6bc428111892",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of the tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490eb6c3-a555-4a31-ae4a-78406914aded",
   "metadata": {},
   "source": [
    "# Operations on Tensors  \n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are described in the pytorch documentation.\n",
    "Each of these operations can be run on the GPU (at typically higher speeds than on a CPU).\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e56e5-46fd-4830-b10b-bccf580b623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e5b67-f997-442e-8758-c47aa64bc7c3",
   "metadata": {},
   "source": [
    "## Standard numpy-like indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd042fd-dcb7-4ce7-9727-4b8b9243483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebeb80f-4530-403d-9b63-ae498082fcb9",
   "metadata": {},
   "source": [
    "**Joining tensors** You can use `torch.cat` to concatenate a sequence of tensors along a given dimension. See also `torch.stack`, another tensor joining operator that\n",
    "is subtly different from `torch.cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4390a5f-a7bc-497c-8f5d-5fd54be227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48281135-bddc-4297-9575-e4745af9c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059346bc-b10e-43dd-91c9-7ccd001ddef8",
   "metadata": {},
   "source": [
    "## Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c4729-faa9-44ba-8db0-faeddaf2158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0539c-3a50-4334-913d-284e2a962dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3a18e-856f-4f10-abb2-979fce29e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a91ac5-8a7e-4363-b6e4-9655d85f8fe2",
   "metadata": {},
   "source": [
    "## **Single-element tensors** \n",
    "If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python\n",
    "numerical value using `item`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed856db-4f72-4634-9698-218f535edd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7703b-6f5b-4556-99d8-200a77e7a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = torch.ones(4, 4, dtype=torch.int8)\n",
    "print(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e2dfa-6eba-4ac8-b684-da3c9c7239a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with the single-element tensors\n",
    "tes_set = tes.sum()\n",
    "print(tes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbe398-6935-43ab-bcda-252b084e3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_set = tes_set.item()\n",
    "print(tes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924bc4d4-2833-48a2-a783-5dff10c27751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tes_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab08d6-c902-41ac-bce4-8f0150d5b216",
   "metadata": {},
   "source": [
    "## **In-place operations**  \n",
    "Operations that store the result into the operand are called in-place. They are denoted by a _sufix. For example: `x.copy_(y)`, `x.t_()`, will change `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b60b07-e7f4-48c8-b5e2-f1b5683a9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da705f-3411-4520-a599-340b94bd2a5e",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> In-place operations save some memory, but can be problematic when computing derivatives because of an immediate  loss of history. Hence, their use is discouraged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f99ce-c2c5-4ee1-b94b-6523eb2af9fd",
   "metadata": {},
   "source": [
    "## Bridge with NumPy\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2d913-42fb-47c2-9712-1f768d143ae5",
   "metadata": {},
   "source": [
    "## Tensor to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bfe73-6e1a-4f46-a0d5-d825dbec30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12849f-576c-49fa-9242-8ffacfc21e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A change in the tensor reflects in the NumPy array.\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5a179-0c52-48e5-836b-78efb6c0dbab",
   "metadata": {},
   "source": [
    "## NumPy array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b2b13-460b-433f-9fe0-74e5d20c7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525abf6d-482d-43bd-af21-31e06eee356e",
   "metadata": {},
   "source": [
    "**Changes in the NummPy array reflects in the tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25a13a-cecc-4eef-8283-16c9655ba058",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca132a-a1e2-48a4-aaf5-9ec559f2e2ff",
   "metadata": {},
   "source": [
    "## Practice tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65bbd8-55ef-423b-b114-5ef8cf9d17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tensors\n",
    "# From value\n",
    "val = 12\n",
    "tensor_from_value = torch.tensor(val)\n",
    "print(val)\n",
    "print(tensor_from_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4496c95-9f51-4dd0-a26c-93254912ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of ones\n",
    "tensor_of_ones = torch.ones(2, 2)\n",
    "print(tensor_of_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f443d74-8726-4ea9-8632-1bd5e4269d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of ones with datatype\n",
    "tensor_of_ones_v2 = torch.ones(2, 2, dtype=torch.int16)\n",
    "print(tensor_of_ones_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470e72a-b37c-4cdb-9469-fcb7d6466994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor of random values between 0 and 1\n",
    "tensor_random = torch.rand(2, 3)\n",
    "print(tensor_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0cfa4-65d5-40fe-b4b8-53e06e4b66a8",
   "metadata": {},
   "source": [
    "# Datasets and DataLoaders  \n",
    "---\n",
    "\n",
    "## Loading a Dataset  \n",
    "Here is an example of how to load the **Fashion-MNIST** dataset from TorchVision. Fashion-MNIST is a dataset of Zalando's article images consisting of 60,000 training samples and 10,000 test samples. Each example comprises a 28x28 grayscale image and an associated label from one of 10 classes.\n",
    "\n",
    "We load the **FashionMNIST Dataset** with the following parameters:  \n",
    "* `root` is the path where the train/test data is stored,\n",
    "* `train` specifics training or test dataset,\n",
    "* `download=True` downloads the data from the internet if it's not available at `root`,\n",
    "* `transform` and `target_transform` specify the feature and the label transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ac9b8-d521-4648-b01d-9942181ad990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223d789-144d-4a55-8a91-3efe247e0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e26d21-0431-4082-8344-f7af6f31d23f",
   "metadata": {},
   "source": [
    "## Iterating and Visualizing the Dataset  \n",
    "We can index `Datasets` manually like a list: `training_data[index]`. We use matplotlib to visualize some samples in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89671d-1be8-4494-a72f-e19dae51ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trowser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplots(rows, cols, i)\n",
    "    plt.title(labels_map[lable])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdbff4-ce78-4a5a-95ce-08f12ba82221",
   "metadata": {},
   "source": [
    "## Creating a Custom Dataset for your files  \n",
    "A custom Dataset class must implement three functions: *__init__, __len__, and __getitem__*. Take a look at this implementations; the FashionMNIST images are stored in a directory `img_dir`, and their labels are stored separately in a CSV file `annotations_file`.  \n",
    "\n",
    "In the next sections, we'll breakdown what's happening in each of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4251e-f5e2-4428-bb45-32dbda57a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89007c4e-7afe-4863-99ad-d8ceea1ac7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cebc008-40d7-4dc4-a949-af46a8f18c20",
   "metadata": {},
   "source": [
    "## `__init__`\n",
    "The '__init__' function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms, this will be covered in more detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485c546-9807-49fc-a7b7-7ddeca646331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8c8f6-589d-4ed7-9bbd-d3d44eaa0d32",
   "metadata": {},
   "source": [
    "## `__len__`  \n",
    "The '__len__' function returns the number of samples in our dataset.  \n",
    "Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c99d57-436c-4657-8d91-c3f65b50cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return len(self.img_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6afa5-0aae-4ed3-a342-4d9f75cb6eb2",
   "metadata": {},
   "source": [
    "## `__getitem__`  \n",
    "The __getitem__ function loads and returns a sample from the dataset at the given `idx`. Based on the index, it identifies the image's location on disk, converts that to a tensor using the `read_image`, retrieves the corresponding label from the csv data in `self.img_labels`, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01cfc3-c7d8-411d-a2a0-711fc6c85a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = read_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1bba15-38cc-445d-9f77-65620ee29f07",
   "metadata": {},
   "source": [
    "## Preparing your data for training with DataLoaders\n",
    "\n",
    "The `Dataset` retrieves our dataset's features and labels one sample at a time. While training a model, we typically want to pass samples in \"minibatches\", reshuffle the data at every epoch to reduce model overfitting, and use Python's `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "`DataLoader` is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f85ee-ef1f-4a17-b119-b0946b6eb78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f13e2-141c-4881-b7cb-41fae10efed3",
   "metadata": {},
   "source": [
    "## Iterate through the DataLoader  \n",
    "We have loaded that dataset into the `DataLoader` and can iterate through the dataset as needed. Each iteration below returns a batch of `train_features` and `train_labels` (containing `batch_size=64` features and labels respectively). Because we specified `shuffle=True`, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa76876-249e-4132-85aa-17767379459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d25a8-711a-4404-b675-a445e2c18d4b",
   "metadata": {},
   "source": [
    "## torch.utils.data\n",
    "At the heart of PyTorch data loading utility is the `torch.utils.data.DataLoader` class. It represents a Python iterable over a dataset, with support for:  \n",
    "- map-style and iterable-style datasets,\n",
    "- customizing data loading order,\n",
    "- automatic batching,\n",
    "- single and multi-process data loading,\n",
    "- automatic memory pinning.\n",
    "\n",
    "These options are configured by the constructor arguments of a `DataLoader`, which has signature\n",
    "\n",
    "```Python\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,  \n",
    "            batch_sampler=None, num_workers=0, collate_fn=None,  \n",
    "            pin_memory=False, drop_last=False, timeout=0,  \n",
    "            worker_init_fn=None, x, prefetch_factor=2,  \n",
    "            peristent_workers=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0070a1f-f5a4-4954-afd7-ac5e33267f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88768f0f-92ea-4da3-ad17-b8b457eb3b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558fcc3-8fd0-4230-8c6f-c270b40b35de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79ec88-3a69-4fef-9db4-92aaca59614f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2417af7-94e4-4f73-8072-1d6f67388018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f52ef0-a67d-4adf-868f-f61a4107f366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c5004-a72b-4938-98e8-ec06062e7b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36cf4e-74d4-4fd7-b5f5-5ce2a8311e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484bf07e-08bb-4745-901c-5789e7993cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6795fd0-d5da-4685-87e2-db2230205cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ee042-2756-49a0-83b8-378f02748f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d93dce-4350-41ec-93ff-e21c5302969f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190a214-58bc-4862-b361-eb1193adf394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6acbf9-195f-46d3-850b-cd1215475be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fd262-7ff3-4f26-9e10-486cb676c3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f4c19-c9f5-4487-a415-733a74decba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72ebc-5a92-41e6-9525-cfffffb65e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88cacd-e67b-49a7-8124-be5c6a44f747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04255bd-4a1b-4800-8715-50dc485ef2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961637b-af18-4521-a749-1353774367af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32373180-ea86-432a-b382-4d7dd4257e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f858dd5-fe6c-4956-b611-986d5a94136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7cf4b-5e7c-41c6-bec7-c27a6e0ffd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb61d7-b1a9-4d45-b289-22dd01f2891f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469786c-344f-44fa-a0cb-409f6815a2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc37dfa-2b1b-4090-9ab3-37425b29299e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea154fcc-b384-4fb5-bac2-0c861bc1755f",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec4323-c73b-4b28-8c6c-dbc670129c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10,\n",
    "                                                 dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822221f7-d8d3-4274-972b-aa630d252e7f",
   "metadata": {},
   "source": [
    "## ToTensor  \n",
    "`ToTensor` converts a PIL image or NumPy `ndarray` into a `FloatTensor`. and scales the image's pixel intensity values in the range[0., 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29302030-1bef-47a7-830e-91d210e1781c",
   "metadata": {},
   "source": [
    "## Lambda Transforms\n",
    "Lambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls `scatter_` which assigns a `value=1` on the index as given by the label `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4f8fd3-9325-4983-87f1-50d1a0da3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46d0936-08ef-4c98-b549-5e4e11889bb0",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e6236-799f-4c65-becb-85238a3af96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526b723-cb3f-4681-81ae-e1cb34dfba39",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "We want to be able to train our model on a hardware accelerator like the GPU or MPS, if available. Let's check to see if `torch.cuda` or `torch.backends.mps` are available, otherwise we use the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2f0c7-e3a9-42a0-bd0a-8d916bbf4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b934b-769b-4927-87c3-ea0b0fcbdd32",
   "metadata": {},
   "source": [
    "## Define the class\n",
    "We define our neural network by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations the operations on input data in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4a4e8-1dfe-4e73-b17c-99231c94018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cd410-af1a-41d9-91ba-c389b6c45c36",
   "metadata": {},
   "source": [
    "We create an instance of `NeuralNetwork`, and move it to the `device`, and print its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e9d55-dfe3-4472-a0c5-4a7facddc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfea50f-421a-4a62-81d5-fd71141fc0e1",
   "metadata": {},
   "source": [
    "To use the model, we pass it the input data. This executes the model's `forward`, along with some `background operations`. Do not call `model.forward()` directly!\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cdcbc-0e21-4de9-b057-63bc494659c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = mm.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffa5918-93aa-4a1f-9ae6-643c3af9edf2",
   "metadata": {},
   "source": [
    "## Model Layers\n",
    "Let's break down the layers of the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9375caf-0cf1-4987-8a8e-b8a7b1eeb7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54046f4-9fa7-4060-b183-ce08a622a393",
   "metadata": {},
   "source": [
    "## nn.Flatten\n",
    "We initialize the `nn.Flatten` layer to convert each 2D 28x28 image into a contagious array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5dea3b-22be-4cc4-ba65-1164839ad4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a09d9-1b48-4a34-8b89-d31910a720ed",
   "metadata": {},
   "source": [
    "## nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c6fc8-d47e-4ec8-9d40-9f624acf8044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2a8a3-1d94-4ff1-bffe-d03882b07f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6e49a-4b29-43f3-a0f9-6ee7a9f7af85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89a841-238e-45f8-aaed-981f465d29ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99ad36-3fe7-4660-9aa8-6323002797ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e97cdd3-c47d-45d4-8078-6dcb3f42b490",
   "metadata": {},
   "source": [
    "# Distributed and Parallel Training Tutorial\n",
    "There are a few ways you can perform distributed training in PyTorch with each method having their advantages in certain use cases:\n",
    "* DistributedDataParallel (DDP)\n",
    "* Fully Shared Data Parallel (FSDP)\n",
    "* Tensor Parallel (TP)\n",
    "* Device Mesh\n",
    "* Remote Procedure Call (RPC) distributed training\n",
    "* Custom Extensions\n",
    "\n",
    "## Getting started with Distributed Data Parallel\n",
    "**Prerequisities:**\n",
    "* PyTorch Distributed Overview\n",
    "* DistributedDataParallel API documents\n",
    "* DistributedDataParallel notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42285d1f-90a9-4d7d-8ba9-387d4c8ed8e4",
   "metadata": {},
   "source": [
    "## Pytorch Distributed Overview\n",
    "This is the overview page for the `torch.distributed` package. The goal of this page is to categorize documents into different topics and briefly describe each of them. If this is your first time building distributed training applications using PyTorch, it is recommended to use this document to navigate to the technology that can best serve your case.\n",
    "\n",
    "## Introduction\n",
    "The PyTorch Distributed library includes a collective of parallelism modules, a communications layer and infrastructure for lauching and debugging large training jobs.\n",
    "\n",
    "## Parallelism APIs\n",
    "These Parallelism Modules offer high -level functionality and compose with existing models:\n",
    "- Distributed Data-Parallel (DDP)\n",
    "- Fully Sharded Data-Parallel Training (FSDP)\n",
    "- Tensor Parallel (TP)\n",
    "- Pipeline Parallel (PP)\n",
    "\n",
    "## Sharding primitives\n",
    "`DTensor` and `DeviceMesh` are primitives used to build parallelism in terms of sharded or replicated tensors on N-dimensional process groups.\n",
    "* `DTensor` represents a tensor that is sharded and/or replicated, and communicates automatically to reshard tensors as needed by operations.\n",
    "* `DeviceMesh` abstracts the accelerator device communicators into a multi-dimensional array, which manages the underlying `ProcessGroup` instances for collective communications in multi-dimensional parallelisms. Try out `Device Mesh Recipe` to learn more.\n",
    "\n",
    "## Communications APIs\n",
    "The `PyTorch distributed communication layer (C10D)` offers both collective communication APIs (e.g., `all_reduce` and `all_gather`) and P2P communication APIs (e.g., `send` and `isend`), which are used under the hood in all of parallelism implementations. `Writing Distributed Applications with PyTorch` shows examples of using c10d communication APIs.\n",
    "\n",
    "## Launcher\n",
    "`torchrun` is a widely-used launcher script, which spawns processes on the local and remote machines for running distributed PyTorch programs.\n",
    "\n",
    "## Applying Parallelism To Scale Your Model\n",
    "Data Parallelism is a widely adopted single-program multiple-data training paradigm where the model is replicated on every model replica computes local gradients for a different set of input data samples, gradients are averaged within the data-parallel communicator group before each optimizer step.\n",
    "\n",
    "Model Parallelism techniques (or sharded Data Parallelism) are required when a model doesn't fit in GPU, and can be combined together to form multi-dimensional (N-D) parallelism techniques.\n",
    "\n",
    "When deciding what parallelism techniques to choose for your model, use these common guidelines:\n",
    "\n",
    "1. Use `DistributedDataParallel (DDP)`, if your model fits in a single GPU but you want to easily scale up training using multiple GPUs.\n",
    "   * Use `torchrun` to launch multiple pytorch processes if you are using more than one node.\n",
    "2. Use `FullyShardedDataParallel (FSDP) ` when your model cannot fit on one GPU.\n",
    "3. Use `Tensor Parallel (TP)` and/or `Pipeline Parallel (PP)` if you reach scaling limitations with FSDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300bb44-b65e-435d-b2b8-5118f85b44b4",
   "metadata": {},
   "source": [
    "# Getting Started with DeviceMesh\n",
    "\n",
    "Prerequisites:\n",
    "* [Distributed Communication Package](https://pytorch.org/docs/stable/distributed.html) - `torch.distributed`\n",
    "\n",
    "Setting up distributed communicators, i.e. NVIDIA Collective Communication Library (NCCL) communicators, for distributed training can pose a signifant challenge. For workloads where users need to compose different parallelisms, users would need to manually set up and manage NCCL communicators (for example, `processGroup`) For each parallism solution. This process could be complicated and susceptible to errors.\n",
    "`DeviceMesh` can simplify this process, making it more manageable and less prone to errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b9886-aecd-49ef-ae6e-02faa95c5032",
   "metadata": {},
   "source": [
    "## What is DeviceMesh\n",
    "`DeviceMesh` is a higher level abstraction that manages `ProcessGroup`. It allows users to effortlessly create inter-node and intra-node process groups without worrying about to set up ranks correctly for different sub process groups. Users can also easily manage the underlying process_groups/devices for multi-dimensional parallelism via `DeviceMesh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c16ea-6c5f-4c55-9321-2a99ad3e8a8a",
   "metadata": {},
   "source": [
    "## Why DeviceMesh is Useful\n",
    "DeviceMesh is useful when working with multi-dimensional parallelism (i.e. 3-D parallel) where parallelism composability is required. For example, when your parallelism solutions require both communication across hosts and within each host. The image above shows that we can create a 2D mesh that connects the devices within each host, and connects each device with its counterpart on the other hosts in a homogenous setup\n",
    "\n",
    "Without DeviceMesh, users would need to manually set up NCCL communicators, cuda devices on each process before applying any parallelism, which could be quite complicated. The following code snippet illustrates a hybrid sharding 2-D Parallel pattern setup without `DeviceMesh`. First, we need to manually calculate the shard group and replicate group. Then, we need to assign the correct shard and replica group to each rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3818687-21c2-4492-9de3-bd5bb46e0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b41e14c5-2464-40b6-8c33-f5c6a7759534",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'RANK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Understand world topology\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRANK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWORLD_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning example on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m in a world with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld_size\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen os>:685\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RANK'"
     ]
    }
   ],
   "source": [
    "# Understand world topology\n",
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "print(f\"Running example on {rank=} in a world with {world_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1301faa-12a5-4d8f-954d-626bbdc87f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674920a7-acfd-4c00-976c-faf0fc6ed2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f03ba7-430b-406b-a39b-0b0450ab104a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f6786-1810-4542-9beb-2c560ac72a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94dd84-9a18-4d68-8ad0-cce42e95e209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
